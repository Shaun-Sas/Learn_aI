{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   8.3252       41.            6.98412698 ...    2.55555556\n",
      "    37.88       -122.23      ]\n",
      " [   8.3014       21.            6.23813708 ...    2.10984183\n",
      "    37.86       -122.22      ]\n",
      " [   7.2574       52.            8.28813559 ...    2.80225989\n",
      "    37.85       -122.24      ]\n",
      " ...\n",
      " [   1.7          17.            5.20554273 ...    2.3256351\n",
      "    39.43       -121.22      ]\n",
      " [   1.8672       18.            5.32951289 ...    2.12320917\n",
      "    39.43       -121.32      ]\n",
      " [   2.3886       16.            5.25471698 ...    2.61698113\n",
      "    39.37       -121.24      ]] [4.526 3.585 3.521 ... 0.923 0.847 0.894]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the dataset\n",
    "housing = fetch_california_housing()\n",
    "X, y = housing.data, housing.target\n",
    "\n",
    "print(X,y)\n",
    "# Split into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class HousingData(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.from_numpy(X.astype(np.float32))\n",
    "        self.y = torch.from_numpy(y.astype(np.float32))\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.shape[0]\n",
    "\n",
    "# Instantiate training and test data\n",
    "train_data = HousingData(X_train, y_train)\n",
    "test_data = HousingData(X_test, y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "batch_size = 64\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch import optim\n",
    "\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 10\n",
    "output_dim = 1\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.layer_1 = nn.Linear(in_features=input_dim, out_features=hidden_dim)\n",
    "        self.layer_2 = nn.Linear(in_features=hidden_dim, out_features=output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer_1(x)\n",
    "        x = nn.functional.relu(x)\n",
    "        x = self.layer_2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "model = NeuralNetwork(input_dim, hidden_dim, output_dim)\n",
    "\n",
    "learning_rate = 0.01\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 | Loss: 0.35698278329169103\n",
      "Epoch: 2 | Loss: 0.3558821883485761\n",
      "Epoch: 3 | Loss: 0.35516296352141896\n",
      "Epoch: 4 | Loss: 0.3542248264041751\n",
      "Epoch: 5 | Loss: 0.35503104649079864\n",
      "Epoch: 6 | Loss: 0.35473911693314863\n",
      "Epoch: 7 | Loss: 0.35477479416897656\n",
      "Epoch: 8 | Loss: 0.35446337451291987\n",
      "Epoch: 9 | Loss: 0.3542928032666881\n",
      "Epoch: 10 | Loss: 0.35408748891060204\n",
      "Epoch: 11 | Loss: 0.3539784643523411\n",
      "Epoch: 12 | Loss: 0.353891329563076\n",
      "Epoch: 13 | Loss: 0.35377255742108815\n",
      "Epoch: 14 | Loss: 0.35376997850010233\n",
      "Epoch: 15 | Loss: 0.35370637312223435\n",
      "Epoch: 16 | Loss: 0.353626261846083\n",
      "Epoch: 17 | Loss: 0.35424806995549213\n",
      "Epoch: 18 | Loss: 0.35409285952268904\n",
      "Epoch: 19 | Loss: 0.35407632525656135\n",
      "Epoch: 20 | Loss: 0.3547125921158712\n",
      "Epoch: 21 | Loss: 0.35482447391706623\n",
      "Epoch: 22 | Loss: 0.35471774345504054\n",
      "Epoch: 23 | Loss: 0.3545716205077578\n",
      "Epoch: 24 | Loss: 0.35437224343544216\n",
      "Epoch: 25 | Loss: 0.3543342321903207\n",
      "Epoch: 26 | Loss: 0.3542007487889248\n",
      "Epoch: 27 | Loss: 0.35404540300091175\n",
      "Epoch: 28 | Loss: 0.3538747356899272\n",
      "Epoch: 29 | Loss: 0.3537984952177236\n",
      "Epoch: 30 | Loss: 0.3536655171232861\n",
      "Epoch: 31 | Loss: 0.3536094007340557\n",
      "Epoch: 32 | Loss: 0.3535558847528196\n",
      "Epoch: 33 | Loss: 0.35358596070270765\n",
      "Epoch: 34 | Loss: 0.35369665602819184\n",
      "Epoch: 35 | Loss: 0.3536941073768889\n",
      "Epoch: 36 | Loss: 0.3535973140665047\n",
      "Epoch: 37 | Loss: 0.35347736040635624\n",
      "Epoch: 38 | Loss: 0.3533448108849247\n",
      "Epoch: 39 | Loss: 0.3532204143480844\n",
      "Epoch: 40 | Loss: 0.3531046851480365\n",
      "Epoch: 41 | Loss: 0.35305820350896366\n",
      "Epoch: 42 | Loss: 0.35296358884786294\n",
      "Epoch: 43 | Loss: 0.35283614856320095\n",
      "Epoch: 44 | Loss: 0.3527407536277885\n",
      "Epoch: 45 | Loss: 0.3526210405317542\n",
      "Epoch: 46 | Loss: 0.35249439428865226\n",
      "Epoch: 47 | Loss: 0.3524208281557247\n",
      "Epoch: 48 | Loss: 0.35241364542628306\n",
      "Epoch: 49 | Loss: 0.3523340999138325\n",
      "Epoch: 50 | Loss: 0.3522685002708851\n",
      "Epoch: 51 | Loss: 0.3521963757730206\n",
      "Epoch: 52 | Loss: 0.35208756174252426\n",
      "Epoch: 53 | Loss: 0.35204721897942937\n",
      "Epoch: 54 | Loss: 0.35193326403746533\n",
      "Epoch: 55 | Loss: 0.3518280800623613\n",
      "Epoch: 56 | Loss: 0.3518034737017769\n",
      "Epoch: 57 | Loss: 0.3516951126677113\n",
      "Epoch: 58 | Loss: 0.35158922700462847\n",
      "Epoch: 59 | Loss: 0.3515304229593617\n",
      "Epoch: 60 | Loss: 0.35142968359814825\n",
      "Epoch: 61 | Loss: 0.35132973893533853\n",
      "Epoch: 62 | Loss: 0.35126707289124776\n",
      "Epoch: 63 | Loss: 0.35121579317120927\n",
      "Epoch: 64 | Loss: 0.351120259782007\n",
      "Epoch: 65 | Loss: 0.35103875401834683\n",
      "Epoch: 66 | Loss: 0.3509503516377538\n",
      "Epoch: 67 | Loss: 0.3508695201508862\n",
      "Epoch: 68 | Loss: 0.35079235210632537\n",
      "Epoch: 69 | Loss: 0.350721884656975\n",
      "Epoch: 70 | Loss: 0.3508954217510993\n",
      "Epoch: 71 | Loss: 0.350809294684057\n",
      "Epoch: 72 | Loss: 0.3507330755211419\n",
      "Epoch: 73 | Loss: 0.350661188192342\n",
      "Epoch: 74 | Loss: 0.3505784028439796\n",
      "Epoch: 75 | Loss: 0.3505132717849056\n",
      "Epoch: 76 | Loss: 0.35045140181823514\n",
      "Epoch: 77 | Loss: 0.3504143170888638\n",
      "Epoch: 78 | Loss: 0.3503528848335612\n",
      "Epoch: 79 | Loss: 0.3505769748605891\n",
      "Epoch: 80 | Loss: 0.35054138973247\n",
      "Epoch: 81 | Loss: 0.3506418347251034\n",
      "Epoch: 82 | Loss: 0.3506263568324732\n",
      "Epoch: 83 | Loss: 0.35062407992694616\n",
      "Epoch: 84 | Loss: 0.35057814498650286\n",
      "Epoch: 85 | Loss: 0.3520358083203156\n",
      "Epoch: 86 | Loss: 0.3522423236272062\n",
      "Epoch: 87 | Loss: 0.3523280507289348\n",
      "Epoch: 88 | Loss: 0.3523866020722565\n",
      "Epoch: 89 | Loss: 0.3523957588274365\n",
      "Epoch: 90 | Loss: 0.3523824490597159\n",
      "Epoch: 91 | Loss: 0.3523507529771458\n",
      "Epoch: 92 | Loss: 0.3523023346618551\n",
      "Epoch: 93 | Loss: 0.3522478347490348\n",
      "Epoch: 94 | Loss: 0.3521787483400585\n",
      "Epoch: 95 | Loss: 0.35211013359914795\n",
      "Epoch: 96 | Loss: 0.35204493551116767\n",
      "Epoch: 97 | Loss: 0.35196950403955746\n",
      "Epoch: 98 | Loss: 0.3519030894366313\n",
      "Epoch: 99 | Loss: 0.3518297072401104\n",
      "Epoch: 100 | Loss: 0.35177451907693186\n",
      "Training Complete\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "from statistics import mean\n",
    "loss_values = []\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    for X, y in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y.unsqueeze(-1))\n",
    "        loss_values.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch: {epoch+1} | Loss: {mean(loss_values)}\")\n",
    "\n",
    "print(\"Training Complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.3644461122842935\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    total_loss = 0\n",
    "    for X, y in test_dataloader:\n",
    "        outputs = model(X)\n",
    "        loss = loss_fn(outputs, y.unsqueeze(-1))\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Test Loss: {total_loss / len(test_dataloader)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
